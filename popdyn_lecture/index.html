---
title: Demographic Inference
layout: presentation
reveal:
    theme: theme.css
---

<section class="titlepage">
  <div class="title">
    Demographic Inference in BEAST 2 and the Extended Bayesian Skyline Plot
  </div>

  <div class="authors">Tim Vaughan</div>
  <div class="institution">Stadler group, D-BSSE, ETH Z&uuml;rich</div>
  <div class="date">24<sup>th</sup> January, 2018</div>
</section>

<section>
  <h1>What is a population?</h1>

  In our context, any group of evolving entities:
  <ul>
    <li>Individual genes</li>
    <li>Microscopic pathogens</li>
    <li>Macroscopic biological organisms</li>
    <li>Groups of organisms</li>
    <li>Entire species</li>
  </ul>
</section>

<section>
  <h2>Demographic inference</h2>

  <!-- Demographic inference: the advantage of sequences -->
  <!-- How sequences yield information about demography -->

  <ul>
    <li>Can use any event which occurs at a rate proportional to the population size.</li>
    <li>Direct samples may fit this bill.</li>
    <li>Genetic samples yield trees: information about past events.</li>
  </ul>
</section>

<section>
  <section class="center">
    <h2>Bayesian Inference</h2>
  </section>

  <section>
    <h2>What is probability?</h2>

    We use the following definition of probability:

    <blockquote>
      For propositions $A$ and $B$, the probabiltiy $P(A|B)$ is
      the degree to which $A$ is believed to be true, on the
      condition that $B$ is true.
    </blockquote>

    <ul>
      <li>Who is doing the believing?</li>
      <li>Probability is subjective.</li>
    </ul>

    <img style="box-shadow:none" data-src="dice_inference.svg">
    </section>

  <section>
    <h2>Manipulating probabilities</h2>

    <p>There are only two rules for manipulating probabilities:</p>

    <br>

    <ol>
      <li>The product rule:
        $$P(A|B,C)P(B|C) = P(A,B|C)$$
        where $A,B$ represents $A$ <b>and</b> $B$.</li>
      <br>

      <li>The sum rule:
        $$P(A|B) + P(\bar{A}|B) = 1$$
        where $\bar{A}$ represents <b>not</b> $A$.</li>
    </ol>

    <br><br>
    <p>That's it! You can now do Bayesian statistics.</p>
  </section>

  <section>
    <h2>Bayes' rule</h2>

    <p>
    Suppose we have a probabilistic model $M$ with parameters $\theta_M$.
      Given data $D$ which we assume has been generated by the model, what can
      we learn about the parameters?</p>

    <br>

    <ul>
      <li> Our model allows us to evaluate $P(D|M,\theta_M)$.
      <li> We want to know $P(\theta_M|D,M)$.
    </ul>

    <br>
    <br>

    <p>Mechanically applying the product rule yields</p>
    <blockquote>
      $$\color{darkred}{P(\theta_M|D,M)} = \frac{\color{yellow}{P(D|M,\theta_M)}\color{darkblue}{P(\theta_M|M)}}{P(D|M)}$$
    </blockquote>

    <p>Terms are named <b style="color:darkred">posterior</b>, <b style="color:#a0a000">likelihood</b> and <b style="color:darkblue">prior</b>.</p>
  </section>

  <section>
    <h2>Bayesian inference</h2>

    <p>Bayes' rule gives us a natural framework for drawing on many sources of information:</p>

    <br>

    <ul class="spaced">
      <li>$\color{darkblue}{P(\theta_M|M}$ describes our state of knowledge of $\theta_M$ <b style="color:darkblue">prior</b> to receiving $D$.  (May still depend on expert knowledge.)</li>
      <li>$\color{#a0a000}{P(D|\theta,M_{\theta})}$ is the <b style="color:#a0a000">likelihood</b> of $\theta_M$ given $D$.  Describes how the data modifies our knowledge of $\theta_M$.</li>
      <li>$\color{red}{P(\theta_M|D,M)}$ describes our state of knowledge <b style="color:darkred">after</b> receiving $D$.
    </ul>

    <br><br>

    <i>The posterior of one analysis may be the prior of a second.</i>
  </section>
</section>

<section>
  <section class="center">
    <h2>Bayesian phylogenetic inference</h2>
  </section>
</section>

<section>
  <h2>Markov chain Monte Carlo (MCMC)</h2>
</section>

<section>
  <h2>Phylodynamic models</h2>
</section>

<section>
  <h2>Kingman's Coalescent</h2>
</section>

<section>
  <h2>Inference of population dynamics</h2>
</section>

<section>
  <h2>Non-parametric inference</h2>
</section>

<section>
  <h2>The Bayesian Skyline Plot</h2>
</section>

<section>
  <h2>The Extended Bayesian Skyline Plot</h2>
</section>

<section>
  <h2>BEAST 2</h2>
</section>

<section class="center">
  <h2>Tutorial: EBSP</h2>
</section>

<section>
  <h2>Tutorial Wrap-up</h2>
</section>

<!--
* Recap: Bayesian phylogenetic inference

* The coalescent framework for phylodynamic inference

* Inference of population dynamics

* Cautionary tails: when the coalescent fails

* Non-parametric inference

* Bayesian skyline plot

* The Extended Bayesian skyline plot

* Introduction to the BEAST 2 ecosystem

* Tutorial: EBSP

* Tutorial wrap-up
-->
